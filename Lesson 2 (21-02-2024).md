`21/02/2024`

**RELIABILITY AND RESILIENCE**

·       **Reliability** is measured in relation to expected or regulated levels of service. It is something that you can manage without human intervention (for example: backup).

·       **Resilience** is the property and/or the performance of the system when the level of service has not been attained for whatever reason, but particularly when subject to unexpected or exceptional disturbances.

|  |  |
| ---- | ---- |
|  |  |
|  | ![A diagram of resilience and resilience<br>Description automatically generated](file:///C:/Users/stefa/AppData/Local/Temp/msohtmlclip1/01/clip_image002.jpg) |

  
Reliability and resilience are related to _risk management_. When the magnitude of a disturbance increases, we must rely on our system resilience. Reliability is for "over time events"; resilience is for "unforeseen events".  
The main goal is to have the greatest sustainability level as possible.

We need to see the entire infrastructure (safety, scalability, reliability, and cybersecurity) to innovatively operate in the field of Distributed Systems/Cybersec. Unfortunately, the costs are high, and the resources are not enough to do everything. Companies appreciate the overall vision of modern services, even if you don't know everything about every field. So, engineers must do the best they can with limited resources, also because perfect solutions require unlimited budget.

![[Pasted image 20240221194742.png]]

Any scalable architecture must avoid:

-          **single point of failure**;

-          **bottlenecks**.

There’s only one solution: **redundancy**. However, the problem with replication is that we need to orchestrate these rereplicas.

Replication has several objects:

-          machines;

-          interconnections;

-          data;

-          people;

-          application processes;

-          …

We will see architecture and applications.

**INTERNET DATA CENTER**

We can have replication at several levels. It is possible to replicate infrastructures (machines, interconnections –managed by system engineers), data/applications/processes (done by computer engineers) and even people (done by managers).

Today everything runs on an Internet Data Center (IDC), the basic infrastructure of any service. IDCs can provide scalable (or, even better, elastic à you use the resources that are needed), reliable and secure services delivered through the Internet. IDC represents the back-end infrastructure of any large company.

The concept of IDC comes from the past. Back in the days (‘70), we had physically protected and refrigerated mainframes. Then, technological evolution brought us into having processors that are more powerful than mainframe ones, leading to faster and cheaper solutions (distributed systems), affordable for any company. Even a single person can run his own server; however, **managing** a distributed system is more difficult than having a single machine. The problem is that we have several machines located everywhere, and we rely too much on the physical distance from machines (the nearest they are, we most I can control/manage them and the less I fear problems that can occur).

Then, the best solution (for large companies, but even for smaller ones) is to have a IDC, which consists in a lots of machine in a single location that are managed by competent people. Instead of having one single computer per rack, we leverage the possibility to have 10/50/100 single machines in a single rack.

An IDC is composed by many elements:

-          Racks;

-          PODs (Performance Optimized Datacenter) - container-type DC, including multiple rackss, data storage devices, network infrastructures, power plant, cooling system. Theyhave a modular approach to DC expansion that uses preconfigured components to provide extra power capacity where it is needed.

-          Storage systems.

-          Virtualization.

-          Advanced internal and external networking.

-          A lot of other support infrastructures (monitoring, power, cooling, physical security, logistics, etc.).

Overall vision of a IDC:

-          **HW** (computing, networking, and storage capacities) – powerful components;

-          **Virtual DC**, by virtualizing the HW layer (we will always use a virtual machine) à its useful because we can maximize the HW usage by allowing lots of users to send requests (and get services ofc);

-          **Services** (BD analysis, business intelligence, DEVOP, file sharing & file synchronization).

The basic ingredient for an IDC is called **blade** server, which is useful for computation. More blade servers can form a single rack, and interconnected racks can form an IDC. Blade servers can also be used for storage purposes.

Another fundamental ingredient is virtualization, which allows us to maximize the HW usage, by having lots of VMs in a single machine.

**INTERNAL NETWORK**

The typical interconnection of a DC has a hierarchical topology (**basic tree**), which has two problems: single point of failure and bottleneck. The alternative is to build a **fat tree**, which resolves these issues. The 1st generation of DCs used the basic tree. Nowadays the goal is to have _agility_/_flexibility_: any virtual server can be dynamically assigned to any service anywhere in the DC. à We don't have to know where the processes are executed, expecially if we have thousands of users.

Unfortunately, conventional datacenter network designs work against agility by **_fragmenting both network and server capacity_** and limiting **_elasticity_** (=the dynamic growing and shrinking of the server pools). Multiple applications run inside a DC, typically with each application hosted on its own set of (potentially virtual) server machines.

A datacenter network supports two types of traffic:

·       **Traffic flowing between external systems and internal servers**. For example, in _video download_ applications, external traffic dominates.

·       **Traffic flowing among internal servers**. For example, in search applications and customized responses, internal traffic dominates because of building and synchronizing instances of the indexes, and inter-process communications.

To support external requests from the Internet, an application is associated with one or more publicly visible and routable IP address(es) to which clients send their requests and from which they receive replies. Inside the DC, the requests are spread by a **specialized HW load balancer** among a pool of front-end servers that process the requests. The IP address to which requests are sent is called a **virtual IP address** (**VIP**), while the IP address of the servers over which the requests are spread are known as **direct IP addresses** (**DIP**).

We want to build **location-independent applications**, which is not possible with a hierarchical network.

(_slide 34 – 35 to see the motivation that lead to the solution to hierarchical networks issues_)

The solution is to have a flat network (any-to-any connectivity): key resources are ALWAYS one hop away, but these solutions cost a lot. With this solution we don't have to care where the machines are.

**SERVER REPLICATION**

The architecture of a medium/large company is so complex that employers often don't know how it's done. This is because the infrastructure has to develop over the years. When building a new architecture we have to build a clean one, by using virtualization and automation and separating HW, virtualization and applications layers.  This guarantees transparency: the users don't have to care on which machine their data is stored!

Verical replication. 3 layers:

1)     **Presentation logic** - front-end server, which is the one that the final users uses;

2)     **Business logic** – application server (NB presentation and business logics are useless without data);

3)     **Transaction server** - back-end server.

(_see slide 45 for horizontal replication)_

Horizontal and vertical replication can be combined. Most web-based services are based on a logical aggregation of VMs in one or multiple DCs.

END-----