                                                                                                                                             21/02/2024

**RELIABILITY AND RESILIENCE**

·       **Reliability** is measured in relation to expected or regulated levels of service. It is something that you can manage without human intervention (for example: backup).

·       **Resilience** is the property and/or the performance of the system when the level of service has not been attained for whatever reason, but particularly when subject to unexpected or exceptional disturbances.

![](file:///C:/Users/stefa/AppData/Local/Temp/msohtmlclip1/01/clip_image002.jpg)
  
Reliability and resilience are related to _risk management_. When the magnitude of a disturbance increases, we must rely on our system resilience. Reliability is for "over time events"; resilience is for "unforeseen events".  
The main goal is to have the greatest sustainability level as possible.

We need to see the entire infrastructure (safety, scalability, reliability, and cybersecurity) to innovatively operate in the field of Distributed Systems/Cybersec. Unfortunately, the costs are high, and the resources are not enough to do everything. Companies appreciate the overall vision of modern services, even if you don't know everything about every field. So, engineers must do the best they can with limited resources, also because perfect solutions require unlimited budget.

![](file:///C:/Users/stefa/AppData/Local/Temp/msohtmlclip1/01/clip_image004.jpg)

Any scalable architecture must avoid:

-          single point of failure;

-          bottlenecks.

There’s only one solution: **redundancy**. However, the problem with replication is that we need to orchestrate these rereplicas.

Replication has several objects:

-          machines;

-          interconnections;

-          data;

-          people;

-          application processes;

-          …

We will see architecture and applications.

**INTERNET DATA CENTER**

We can have replication at several levels. It is possible to replicate infrastructures (machines, interconnections –managed by system engineers), data/applications/processes (done by computer engineers) and even people (done by managers).

Today everything runs on an Internet Data Center (IDC), the basic infrastructure of any service. IDCs can provide scalable (or, even better, elastic à you use the resources that are needed), reliable and secure services delivered through the Internet. IDC represents the back-end infrastructure of any large company.

The concept of IDC comes from the past. Back in the days (‘70), we had physically protected and refrigerated mainframes. Then, technological evolution brought us into having processors that are more powerful than mainframe ones, leading to faster and cheaper solutions (distributed systems), affordable for any company. Even a single person can run his own server; however, **managing** a distributed system is more difficult than having a single machine. The problem is that we have several machines located everywhere, and we rely too much on the physical distance from machines (the nearest they are, we most I can control/manage them and the less I fear problems that can occur).

Then, the best solution (for large companies, but even for smaller ones) is to have a IDC, which consists in a lots of machine in a single location that are managed by competent people. Instead of having one single computer per rack, we leverage the possibility to have 10/50/100 single machines in a single rack.

An IDC is composed by many elements:

-          Racks;

-          PODs (Performance Optimized Datacenter) - container-type DC, including multiple rackss, data storage devices, network infrastructures, power plant, cooling system. Theyhave a modular approach to DC expansion that uses preconfigured components to provide extra power capacity where it is needed.

-          Storage systems.

-          Virtualization.

-          Advanced internal and external networking.

-          A lot of other support infrastructures (monitoring, power, cooling, physical security, logistics, etc.).

Overall vision of a IDC:

-          **HW** (computing, networking, and storage capacities) – powerful components;

-          **Virtual DC**, by virtualizing the HW layer (we will always use a virtual machine) à its useful because we can maximize the HW usage by allowing lots of users to send requests (and get services ofc);

-          **Services** (BD analysis, business intelligence, DEVOP, file sharing & file synchronization).

The basic ingredient for an IDC is called **blade** server, which is useful for computation. More blade servers can form a single rack, and interconnected racks can form an IDC. Blade servers can also be used for storage purposes.

Another fundamental ingredient is virtualization, which allows us to maximize the HW usage, by having lots of VMs in a single machine.

**INTERNAL NETWORK**

The typical interconnection of a DC has a hierarchical topology (**basic tree**), which has two problems: single point of failure and bottleneck. The alternative is to build a **fat tree**, which resolves these issues. The 1st generation of DCs used the basic tree. Nowadays the goal is to have _agility_/_flexibility_: any virtual server can be dynamically assigned to any service anywhere in the DC. à We don't have to know where the processes are executed, especially if we have thousands of users.

![](file:///C:/Users/stefa/AppData/Local/Temp/msohtmlclip1/01/clip_image006.jpg)

Unfortunately, conventional datacenter network designs work against agility by **_fragmenting both network and server capacity_** and limiting **_elasticity_** (=the dynamic growing and shrinking of the server pools). Multiple applications run inside a DC, typically with each application hosted on its own set of (potentially virtual) server machines.

A datacenter network supports two types of traffic:

·       **Traffic flowing between external systems and internal servers**. For example, in _video download_ applications, external traffic dominates.

·       **Traffic flowing among internal servers**. For example, in search applications and customized responses, internal traffic dominates because of building and synchronizing instances of the indexes, and inter-process communications.

To support external requests from the Internet, an application is associated with one or more publicly visible and routable IP address(es) to which clients send their requests and from which they receive replies. Inside the DC, the requests are spread by a **specialized HW load balancer** among a pool of front-end servers that process the requests. The IP address to which requests are sent is called a **virtual IP address** (**VIP**), while the IP address of the servers over which the requests are spread are known as **direct IP addresses** (**DIP**).

We want to build **location-independent applications**, which is not possible with a hierarchical network.

The possible solutions for agility are:

·      **Location independent addressing** – Services should use location-independent addresses that decouple the server’s location in the datacenter from its address. This could enable any server to become part of any server pool while simplifying configuration management.

·      **Uniform bandwidth and latency** – If the available bandwidth between two servers is not dependent on where they are located, then the servers for a given service could be distributed arbitrarily in the data center without fear of running into bandwidth bottleneck points.

Uniform bandwidth, combined with uniform latency between any two servers would allow services to achieve the same performance regardless of the location of their servers.

**ATTENTION:** server resources in a datacenter based on switches and routers do not represent a uniform pool! In fact, applications on the same rack can communicate much faster than applications on different racks, because these ones have to pass through switches/routers.

![](file:///C:/Users/stefa/AppData/Local/Temp/msohtmlclip1/01/clip_image008.jpg)

**PROBLEMS OF HIERARCHICAL NETWORKS**

·       **Problem 1 – Static network assignment  
To support internal traffic within the DC, individual applications are mapped to specific physical switches and routers, relying heavily on VLANs and layer-3 based VLAN spanning to cover the servers dedicated to the application. The positive consequence is that we have a high degree of performance and security isolation. The negative consequences are:

o   VLANs are often **policy-overloaded**, integrating traffic management, security, and performance isolation;

o   VLANs and large server pools **concentrate traffic on links high in the tree**, where links and routers are highly overbooked.

·       **Problem 2 – Fragmentation of resources**

o   **Some** **load balancing techniques (half NAT)** require that all IPs in a pool of a Virtual IP be in the same layer 2 domain à PROBLEM: if an application grows and requires more front-end servers, it cannot use available servers in other layer 2 domains, ultimately resulting in fragmentation and under-utilization of resources.

o   **Load balancing via full NAT** allows servers to be spread across multiple layer 2 domains. PROBLEM: the servers do not see the client IP, which is unacceptable because servers use the client IP for everything, from data mining and response customization to regulatory compliance.

·       **Problem 3 – Poor server-to-server connectivity  
The communication between servers in different layer 2 domains must go through the layer 3 network portions

o   Layer 3 ports are significantly more expensive than layer 2 ports (cost of supporting large buffers, marketplace factors). As a result, these links are typically oversubscribed by factors of 10:1 to 80:1 à **The bandwidth available between servers in different parts of the datacenter can be limited.**

o   **Managing the scarce internal bandwidth is an interesting global optimization problem**: servers from all applications must be placed with great care to ensure that the sum of their traffic does not saturate any of the network links à Unfortunately, achieving this level of coordination between continuously changing applications is impossible in practice.

The solution is to have a flat network (any-to-any connectivity): key resources are ALWAYS one hop away, but these solutions cost a lot. With this solution we don't have to care where the machines are.

![](file:///C:/Users/stefa/AppData/Local/Temp/msohtmlclip1/01/clip_image010.jpg)

**SERVER REPLICATION**


The architecture of a medium/large company is so complex that employers often don't know how it's done. This is because the infrastructure has to develop over the years. When building a new architecture, we have to build a clean one, by using virtualization and automation and separating HW, virtualization and applications layers.  This guarantees transparency: the users don't have to care on which machine their data is stored!

**Verical replication** – 3 layers:

1)     **Presentation logic** - front-end server, which is the one that the final users uses;

2)     **Business logic** – application server (NB presentation and business logics are useless without data);

![](file:///C:/Users/stefa/AppData/Local/Temp/msohtmlclip1/01/clip_image012.jpg)
3)     **Transaction server** - back-end server.

![](file:///C:/Users/stefa/AppData/Local/Temp/msohtmlclip1/01/clip_image014.jpg)

**Horizontal replication** – 2 layers:

1)     **Load balancer (VIP)**;

2)     **Front-end servers.**

Horizontal and vertical replication can be combined. Most web-based services are based on a logical aggregation of VMs in one or multiple DCs.

![](file:///C:/Users/stefa/AppData/Local/Temp/msohtmlclip1/01/clip_image016.jpg)